{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlN3Gbb/M+amgQDugbgoLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatCat776/InterviewBot/blob/main/interview_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Job Interview Bot\n"
      ],
      "metadata": {
        "id": "3r8L7w0ZWUEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "2rlU2bTEyPxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "BIR9rVbpyZNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet google-genai pandas==2.2.2\n",
        "# not sure I really need pandas, colab appears to only support old pandas\n"
      ],
      "metadata": {
        "id": "YXtPGbBffejR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u7tV7GkDIi_S"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Gemini"
      ],
      "metadata": {
        "id": "q6G85BQoWhTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# create a google api key and save it as a secret in colab before running next\n",
        "# line\n",
        "GOOGLE_API_KEY = userdata.get('AI_STUDIO_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "model_name = \"gemini-2.5-flash-lite\"\n"
      ],
      "metadata": {
        "id": "ETKSChDxUCKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure google drive"
      ],
      "metadata": {
        "id": "ahwxWv2sZ0L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# next line requires granting permissions to access drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OnWj_cp9Z2Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo for multi-turn chat"
      ],
      "metadata": {
        "id": "2fGVMBK2OC_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai import configure\n",
        "from google.generativeai import GenerativeModel\n",
        "configure(api_key=GOOGLE_API_KEY)\n",
        "my_instructions='''Provide short answers to each question.'''\n",
        "#TODO: add safety_settings as input to next line\n",
        "model = GenerativeModel(model_name=model_name, system_instruction=my_instructions)\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "user_message_1 = \"Hello, what is the capital of France?\"\n",
        "response_1 = chat.send_message(user_message_1)\n",
        "print(f\"User: {user_message_1}\")\n",
        "print(f\"Gemini: {response_1.text}\")\n",
        "user_message_2 = \"Hello, what the largest airport is in that city?\"\n",
        "response_2 = chat.send_message(user_message_2)\n",
        "print(f\"User: {user_message_2}\")\n",
        "print(f\"Gemini: {response_2.text}\")"
      ],
      "metadata": {
        "id": "Ld6QbdOKOGSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-turn Chat with job posting"
      ],
      "metadata": {
        "id": "iiDkEUmYaVAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content of the text file\n",
        "with open(\"/content/drive/My Drive/job_posting.txt\", \"r\") as f:\n",
        "    job_posting_content = f.read()\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "user_message_1 = \"Here is a copy of the job posting: \" + job_posting_content + \"What is the job title?\"\n",
        "response_1 = chat.send_message(user_message_1)\n",
        "print(f\"User: {user_message_1}\")\n",
        "print(f\"Gemini: {response_1.text}\")\n",
        "user_message_2 = \"What is the salary range?\"\n",
        "response_2 = chat.send_message(user_message_2)\n",
        "print(f\"User: {user_message_2}\")\n",
        "print(f\"Gemini: {response_2.text}\")"
      ],
      "metadata": {
        "id": "tsTL-gVVaXoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Job Posting, text method"
      ],
      "metadata": {
        "id": "Xgo_dZVHX8ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read the content of the text file\n",
        "# with open(\"/content/drive/My Drive/job_posting.txt\", \"r\") as f:\n",
        "#     job_posting_content = f.read()\n",
        "\n",
        "# contents = [\n",
        "#     types.Content(\n",
        "#       role=\"user\",\n",
        "#       parts=[\n",
        "#         types.Part.from_text(text=\"\"\"Tell me about the document\"\"\"),\n",
        "#         types.Part.from_text(text=job_posting_content), # Include the text content directly\n",
        "#         types.Part.from_text(text=\"\"\"Say Boo when done\"\"\")\n",
        "#       ]\n",
        "#     )\n",
        "#   ]\n",
        "\n",
        "# # # Response as a stream\n",
        "# #for chunk in client.models.generate_content_stream(\n",
        "# #    model = model_name,\n",
        "# #    contents = contents,\n",
        "# #    config = generate_content_config,\n",
        "# #    ):\n",
        "# #    print(chunk.text, end=\"\")\n",
        "\n",
        "# # Respond as a single answer\n",
        "# response = client.models.generate_content(\n",
        "#     model=model_name, contents=contents\n",
        "# )\n",
        "# print(response.text)\n",
        "\n",
        "# # next answer doesn't have job posting available to it.\n",
        "# # TODO: do I need to change to a continuous chat format?\n",
        "# response = client.models.generate_content(\n",
        "#     model=model_name, contents=\"\"\"what is the salary range?\"\"\"\n",
        "# )\n",
        "\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "JiTOzj5YX7oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions that use my resume, PDF method.\n",
        "Pulling content from google drive and github doesn't seem to work in colab the same way it does in Virtex AI. For some reason, I have to convert the pdf to txt, then I can use it here."
      ],
      "metadata": {
        "id": "UeWuge3Od_iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "fyKJ1O43pt5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def convert_pdf_to_txt_pdfplumber(pdf_path, txt_path):\n",
        "    \"\"\"Converts a PDF file to a plain text file using pdfplumber.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf, open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for page in pdf.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    f.write(text + '\\n')  # Add a newline to separate pages\n",
        "        print(f\"Text successfully extracted from {pdf_path} to {txt_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "# convert_pdf_to_txt_pdfplumber(\"input.pdf\", \"output.txt\")\n",
        "\n",
        "def get_text_from_pdf(pdf_path):\n",
        "  convert_pdf_to_txt_pdfplumber(\"/content/drive/My Drive/My_Resume.pdf\", \"/content/drive/My Drive/temp.txt\")\n",
        "  with open(\"/content/drive/My Drive/temp.txt\", \"r\") as f:\n",
        "    text_resume_content = f.read()\n",
        "    return text_resume_content"
      ],
      "metadata": {
        "id": "_13kT-fKpmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi turn chat method\n",
        "text_resume_content = get_text_from_pdf(\"/content/drive/My Drive/resume_as_txt.txt\")\n",
        "\n",
        "user_message = \"Here is a copy of my resume: \" + text_resume_content + \"Am I a good fit for this job?\"\n",
        "response = chat.send_message(user_message)\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"Gemini: {response.text}\")\n"
      ],
      "metadata": {
        "id": "HZSwV9X9bviU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Single prompt question method\n",
        "# text_resume_content = get_text_from_pdf(\"/content/drive/My Drive/resume_as_txt.txt\")\n",
        "# # Don't know why, but printing the resume content is required\n",
        "# #   maybe I could do some other interaction with the variable instead?\n",
        "# #   sleep doesn't work, but print does\n",
        "# print(text_resume_content)\n",
        "# print(\"end of resume content\")\n",
        "\n",
        "# contents = [\n",
        "#     types.Content(\n",
        "#       role=\"user\",\n",
        "#       parts=[\n",
        "#         types.Part.from_text(text=\"\"\"Tell me about the document\"\"\"),\n",
        "#         types.Part.from_text(text=text_resume_content), # Include the text content directly\n",
        "#         types.Part.from_text(text=\"\"\"Say Boo PDF when done\"\"\")\n",
        "#       ]\n",
        "#     )\n",
        "#   ]\n",
        "\n",
        "\n",
        "\n",
        "# for chunk in client.models.generate_content_stream(\n",
        "#     model = model_name,\n",
        "#     contents = contents,\n",
        "#     ):\n",
        "#     print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "id": "sa9znrOoeB3Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static model settings"
      ],
      "metadata": {
        "id": "ZEX3sM9HWpGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: move resume import here\n",
        "my_instructions = \"\"\"\n",
        "You are a job applicant in a job interview. This is your resume:\n",
        "\"\"\" + text_resume_content + \"\"\" This is the job posting: \"\"\" + job_posting_content\n",
        "\n",
        "generate_content_config = types.GenerateContentConfig(\n",
        "    temperature = .5,\n",
        "    top_p = 0.95,\n",
        "    max_output_tokens = 2048,\n",
        "    safety_settings = [types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    )],\n",
        "    system_instruction=[types.Part.from_text(text=my_instructions)],\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "      thinking_budget=0,\n",
        "    ),\n",
        "  )"
      ],
      "metadata": {
        "id": "8PUHeawVOpGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start up model"
      ],
      "metadata": {
        "id": "qSSM-Qisdm7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: add safety_settings as input to next line\n",
        "model = GenerativeModel(model_name=model_name, system_instruction=my_instructions)\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "def ask_question(question):\n",
        "  user_message = question\n",
        "  response = chat.send_message(user_message)\n",
        "  print(f\"User: {user_message}\")\n",
        "  print(f\"Gemini: {response.text}\")"
      ],
      "metadata": {
        "id": "FTsxAc6RdpdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interview Questions"
      ],
      "metadata": {
        "id": "9rrT8WFLXdyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = \"What is your name?\"\n",
        "response = chat.send_message(user_message)\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"Gemini: {response.text}\")\n",
        "\n",
        "ask_question(\"Why are you a good fit for this job?\")\n",
        "ask_question(\"What is your salary expectation?\")"
      ],
      "metadata": {
        "id": "Euwd46KFd57S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contents = [\n",
        "#     types.Content(\n",
        "#       role=\"user\",\n",
        "#       parts=[\n",
        "#         types.Part.from_text(text=\"\"\"Please introduce yourself\"\"\")\n",
        "#       ]\n",
        "#     )\n",
        "#   ]"
      ],
      "metadata": {
        "id": "NjL36CO3WO1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Questions"
      ],
      "metadata": {
        "id": "wuLJCplbWQuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for chunk in client.models.generate_content_stream(\n",
        "#     model = model_name,\n",
        "#     contents = contents,\n",
        "#     config = generate_content_config,\n",
        "#     ):\n",
        "#     print(chunk.text, end=\"\")"
      ],
      "metadata": {
        "id": "Oea75GRGVTm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}